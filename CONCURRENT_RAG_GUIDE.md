# Concurrent RAG Population Guide

## ğŸš€ 5x Faster RAG Population with Multiple API Keys

This system uses **5 PubMed API keys concurrently** to populate your RAG database **5x faster**.

---

## ğŸ“Š Performance Comparison

| Configuration | Requests/Second | Time for 10,000 articles |
|--------------|----------------|-------------------------|
| Single API Key | 3 req/sec | ~5-6 hours |
| 5 API Keys (Concurrent) | 50 req/sec | ~1 hour |

---

## ğŸ”‘ Step 1: Get Your API Keys

1. Go to https://www.ncbi.nlm.nih.gov/account/
2. Sign in or create an NCBI account
3. Navigate to "Settings" â†’ "API Key Management"
4. Create 5 API keys
5. Copy each key

---

## âš™ï¸ Step 2: Configure API Keys

### Option A: Using the Setup Script (Recommended)

```bash
cd /Users/user/AI-Vision-Chatbot
./backend/scripts/setup_api_keys.sh
```

Enter your 5 API keys when prompted.

### Option B: Manual Configuration

Add to your `.env` file:

```bash
PUBMED_API_KEY_1=your_first_api_key_here
PUBMED_API_KEY_2=your_second_api_key_here
PUBMED_API_KEY_3=your_third_api_key_here
PUBMED_API_KEY_4=your_fourth_api_key_here
PUBMED_API_KEY_5=your_fifth_api_key_here
```

---

## ğŸ—ï¸ Step 3: Rebuild Docker Containers

```bash
cd /Users/user/AI-Vision-Chatbot
docker-compose down
docker-compose up -d --build backend celery_worker
```

---

## ğŸš€ Step 4: Run Concurrent Population

```bash
# Run in foreground (see live progress)
docker-compose exec backend python scripts/production_rag_populator.py

# Run in background
nohup docker-compose exec -T backend python scripts/production_rag_populator.py > /tmp/rag_population.log 2>&1 &

# Monitor progress
tail -f /tmp/rag_population.log
```

---

## ğŸ“Š Monitor Progress

### Check Articles Downloaded

```bash
docker-compose exec -T backend python -c "
import json
p = json.load(open('/data/rag_progress.json'))
print(f'âœ“ Completed: {len(p[\"completed_queries\"])}/248 queries')
print(f'âœ“ Downloaded: {len(p[\"downloaded_pmids\"])} articles')
print(f'âœ“ Indexed: {len(p[\"indexed_pmids\"])} articles')
"
```

### Check ChromaDB Collection Size

```bash
docker-compose exec -T backend python -c "
from app.rag.vector_store import vector_store_manager
collection = vector_store_manager.get_collection('pubmed_vision_research')
print(f'ğŸ“š Total chunks indexed: {collection.count()}')
"
```

### View Worker Activity

```bash
# Watch the log file
tail -f /tmp/rag_population.log | grep "Worker"

# Check current processing
docker-compose logs --tail=50 backend | grep "ğŸ”"
```

---

## ğŸ¯ How It Works

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Concurrent RAG Populator              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Worker 1 â”‚  â”‚ Worker 2 â”‚  â”‚ Worker 3 â”‚ ... â”‚
â”‚  â”‚ API Key1 â”‚  â”‚ API Key2 â”‚  â”‚ API Key3 â”‚     â”‚
â”‚  â”‚ 10 req/s â”‚  â”‚ 10 req/s â”‚  â”‚ 10 req/s â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚             â”‚             â”‚            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                     â”‚                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚   PubMed     â”‚                 â”‚
â”‚              â”‚   Search     â”‚                 â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚   Download   â”‚                 â”‚
â”‚              â”‚   Articles   â”‚                 â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚    Chunk &   â”‚                 â”‚
â”‚              â”‚    Index     â”‚                 â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚   ChromaDB   â”‚                 â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Features

1. **Round-Robin Distribution**: Queries are distributed evenly across workers
2. **Rate Limiting**: Each worker respects 10 req/sec limit per API key
3. **Concurrent Processing**: Multiple queries processed simultaneously
4. **Progress Tracking**: Shared progress file with thread-safe updates
5. **Fault Tolerance**: Automatic retries with exponential backoff
6. **Resume Capability**: Picks up where it left off after interruption

### Worker Distribution Example

```
Query 1  â†’ Worker 1 (API Key 1)
Query 2  â†’ Worker 2 (API Key 2)
Query 3  â†’ Worker 3 (API Key 3)
Query 4  â†’ Worker 4 (API Key 4)
Query 5  â†’ Worker 5 (API Key 5)
Query 6  â†’ Worker 1 (API Key 1)  â† Round-robin restart
Query 7  â†’ Worker 2 (API Key 2)
...
```

---

## ğŸ“ˆ Expected Results

### With 5 API Keys

- **Total Capacity**: 50 requests/second
- **Queries**: 248 search terms
- **Articles per Query**: ~500 articles
- **Total Articles**: ~124,000 articles
- **Estimated Time**: ~60 minutes
- **PDFs Downloaded**: ~2,000+ full-text papers

### Progress Output

```
[Worker 1] [Q1/248] ğŸ” retinal degeneration
======================================================================
  ğŸ“¡ Searching PubMed...
  âœ“ Found 500 articles
  â„¹ï¸  450 new articles to download
  â¬‡ï¸  Downloading with Worker 1...
  Progress: 50/450 articles processed
  Progress: 100/450 articles processed
  ...
  âœ“ Downloaded 450 articles (12 with PDFs)
  ğŸ“ Chunking documents...
  âœ“ Created 1,203 chunks
  ğŸ’¾ Indexing in ChromaDB...
  âœ… Indexed 1,203 chunks

  ğŸ“Š Progress: 1/248 queries | 450 articles | 12 PDFs | 1203 indexed
```

---

## ğŸ› ï¸ Troubleshooting

### Issue: "No API keys found"

**Solution**: Check your `.env` file has the keys:
```bash
grep "PUBMED_API_KEY" .env
```

### Issue: "HTTP Error 429: Too Many Requests"

**Solution**: This is normal. The system automatically retries with backoff.

### Issue: Slow progress

**Solution**: 
1. Check all 5 API keys are configured
2. Verify workers are running: `docker-compose logs backend | grep Worker`
3. Ensure good internet connection

### Issue: ChromaDB connection errors

**Solution**:
```bash
docker-compose restart chromadb
docker-compose restart backend
```

---

## ğŸ‰ After Population Complete

### Verify Data

```bash
# Check total indexed chunks
docker-compose exec -T backend python -c "
from app.rag.vector_store import vector_store_manager
collection = vector_store_manager.get_collection('pubmed_vision_research')
print(f'Total chunks: {collection.count()}')
"

# Check progress file
docker-compose exec backend cat /data/rag_progress.json | jq
```

### Backup Your Data

```bash
cd /Users/user/AI-Vision-Chatbot
docker-compose exec -T backend tar czf /tmp/data_backup.tar.gz /data
docker cp vision_backend:/tmp/data_backup.tar.gz ./data_backup_complete.tar.gz
```

### Test RAG System

```bash
# Test a query
docker-compose exec -T backend python -c "
from app.rag.vector_store import vector_store_manager
results = vector_store_manager.search('age-related macular degeneration treatment', k=5)
for i, doc in enumerate(results, 1):
    print(f'{i}. {doc.metadata.get(\"title\", \"Untitled\")}')
"
```

---

## ğŸ’¡ Pro Tips

1. **Run overnight**: Let it run for 1-2 hours uninterrupted
2. **Monitor disk space**: Ensure 5+ GB available
3. **Use tmux/screen**: Keep session alive if SSH
4. **Check logs**: `tail -f /tmp/rag_population.log` for real-time monitoring
5. **Resume anytime**: Safe to Ctrl+C and restart - progress is saved

---

## ğŸ“ Rate Limit Compliance

This system is fully compliant with NCBI E-utilities guidelines:

- âœ… 10 requests/second per API key (enforced by rate limiter)
- âœ… Proper API key usage (each key tracked separately)
- âœ… Email configured in Entrez
- âœ… Exponential backoff on errors
- âœ… Respectful retry logic

**Reference**: https://www.ncbi.nlm.nih.gov/books/NBK25497/

---

## ğŸš€ Quick Start (TL;DR)

```bash
# 1. Add API keys to .env
cd /Users/user/AI-Vision-Chatbot
./backend/scripts/setup_api_keys.sh

# 2. Rebuild
docker-compose up -d --build backend

# 3. Run
docker-compose exec backend python scripts/production_rag_populator.py

# 4. Monitor
tail -f /tmp/rag_population.log
```

Done! Your RAG database will be populated 5x faster! ğŸ‰

